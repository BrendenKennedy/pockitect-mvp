{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pockitect Colab Training (Unsloth)\n",
        "\n",
        "This notebook fine-tunes the model on a GPU-backed Colab runtime using the project training data.\n",
        "\n",
        "**Before you start:** Runtime → Change runtime type → **GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Clone the repo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf pockitect-mvp\n",
        "!git clone https://github.com/BrendenKennedy/pockitect-mvp.git\n",
        "%cd /content/pockitect-mvp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Install dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) (Optional) Validate training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/validate_training_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) (Optional) Regenerate JSONL if you changed training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/convert_to_finetuning_format.py --format unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory-optimized for T4 (15GB VRAM)\n",
        "!python tools/finetune_model.py \\\n",
        "    --model unsloth/Qwen2.5-3B \\\n",
        "    --train data/finetuning/train_unsloth.jsonl \\\n",
        "    --epochs 3 \\\n",
        "    --batch-size 1 \\\n",
        "    --gradient-accumulation 8 \\\n",
        "    --max-seq-len 1024 \\\n",
        "    --early-stopping \\\n",
        "    --early-stopping-patience 3 \\\n",
        "    --eval-steps 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Export to GGUF for Ollama\n",
        "\n",
        "This merges the LoRA adapter with the base model and exports to GGUF format for use with Ollama."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from unsloth import FastLanguageModel\n",
        "\n",
        "# Load the fine-tuned model\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"data/finetuning/output/adapter\",\n",
        "    max_seq_length=1024,\n",
        "    dtype=None,\n",
        "    load_in_4bit=True,\n",
        ")\n",
        "\n",
        "# Save as GGUF (Q4_K_M is a good balance of size and quality)\n",
        "model.save_pretrained_gguf(\n",
        "    \"data/finetuning/output/gguf\",\n",
        "    tokenizer,\n",
        "    quantization_method=\"q4_k_m\"\n",
        ")\n",
        "print(\"✅ GGUF export complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7) Download the GGUF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Find and download the GGUF file\n",
        "import glob\n",
        "from google.colab import files\n",
        "\n",
        "gguf_files = glob.glob(\"data/finetuning/output/gguf/*.gguf\")\n",
        "if gguf_files:\n",
        "    print(f\"Downloading: {gguf_files[0]}\")\n",
        "    files.download(gguf_files[0])\n",
        "else:\n",
        "    print(\"No GGUF file found. Check the export step.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Create Ollama model locally\n",
        "\n",
        "After downloading the GGUF file, run on your local machine:\n",
        "\n",
        "```bash\n",
        "# Create a Modelfile\n",
        "echo 'FROM ./pockitect-unsloth.Q4_K_M.gguf' > Modelfile\n",
        "echo 'PARAMETER temperature 0.2' >> Modelfile\n",
        "\n",
        "# Create the Ollama model\n",
        "ollama create pockitect:finetuned -f Modelfile\n",
        "\n",
        "# Test it\n",
        "ollama run pockitect:finetuned \"I need a simple blog server\"\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
