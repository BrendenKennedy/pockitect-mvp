{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Pockitect Colab Training (Unsloth)\n",
        "\n",
        "This notebook fine-tunes the model on a GPU-backed Colab runtime using the project training data.\n",
        "\n",
        "**Before you start:** Runtime â†’ Change runtime type â†’ **GPU**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Clone the Repository\n",
        "\n",
        "First, we clone the pockitect-mvp repository into the Colab environment. This brings in all the project code, training data, and scripts needed for fine-tuning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!rm -rf pockitect-mvp\n",
        "!git clone https://github.com/BrendenKennedy/pockitect-mvp.git\n",
        "%cd /content/pockitect-mvp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies\n",
        "\n",
        "Install Unsloth (a fast fine-tuning library) and all project dependencies. Unsloth is optimized for Colab and provides significant speed improvements for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
        "!pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Validate Training Data\n",
        "\n",
        "Before training, we validate the training data to ensure it's properly formatted and contains no errors. This helps catch issues early before starting the expensive fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/validate_training_data.py"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Convert to Fine-tuning Format\n",
        "\n",
        "Convert the training data into Unsloth's preferred JSONL format. This format is optimized for efficient loading during training and ensures the data is structured correctly for the fine-tuning process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!python tools/convert_to_finetuning_format.py --format unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Fine-tune the Model\n",
        "\n",
        "This is the main training step. We fine-tune the Qwen2.5-3B model using the converted training data. The configuration is optimized for T4 GPUs (15GB VRAM) with:\n",
        "\n",
        "- **Batch size**: 1 (to fit in memory)\n",
        "- **Gradient accumulation**: 8 (effective batch size of 8)\n",
        "- **Max sequence length**: 1024 tokens\n",
        "- **Early stopping**: Stops training if validation loss doesn't improve for 3 evaluations\n",
        "- **Evaluation steps**: Validates every 25 training steps\n",
        "\n",
        "This step may take 30-60 minutes depending on your GPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Memory-optimized for T4 (15GB VRAM)\n",
        "!python tools/finetune_model.py \\\n",
        "    --model unsloth/Qwen2.5-3B \\\n",
        "    --train data/finetuning/train_unsloth.jsonl \\\n",
        "    --epochs 3 \\\n",
        "    --batch-size 1 \\\n",
        "    --gradient-accumulation 8 \\\n",
        "    --max-seq-len 1024 \\\n",
        "    --early-stopping \\\n",
        "    --early-stopping-patience 3 \\\n",
        "    --eval-steps 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 6: Export to GGUF Format\n",
        "\n",
        "After training completes, we export the fine-tuned model to GGUF format (Q4_K_M quantization). This format:\n",
        "\n",
        "- Reduces model size significantly (from ~6GB to ~2GB)\n",
        "- Maintains good quality with minimal performance loss\n",
        "- Is compatible with Ollama and other inference engines\n",
        "- Can run efficiently on consumer hardware\n",
        "\n",
        "The exported GGUF file will be saved in `data/finetuning/output/gguf/`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# First check if the adapter exists\n",
        "adapter_path = \"data/finetuning/output/adapter\"\n",
        "if not os.path.exists(adapter_path):\n",
        "    print(f\"âŒ Adapter not found at {adapter_path}\")\n",
        "    print(\"Make sure training completed successfully (step 5)\")\n",
        "else:\n",
        "    print(f\"âœ… Found adapter at {adapter_path}\")\n",
        "    print(\"Files:\", os.listdir(adapter_path))\n",
        "\n",
        "    from unsloth import FastLanguageModel\n",
        "\n",
        "    # Load the fine-tuned model\n",
        "    print(\"\\nðŸ“¥ Loading fine-tuned model...\")\n",
        "    model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "        model_name=adapter_path,\n",
        "        max_seq_length=1024,\n",
        "        dtype=None,\n",
        "        load_in_4bit=True,\n",
        "    )\n",
        "\n",
        "    # Save as GGUF (Q4_K_M is a good balance of size and quality)\n",
        "    print(\"\\nðŸ“¦ Exporting to GGUF (this may take a few minutes)...\")\n",
        "    output_dir = \"data/finetuning/output/gguf\"\n",
        "    model.save_pretrained_gguf(\n",
        "        output_dir,\n",
        "        tokenizer,\n",
        "        quantization_method=\"q4_k_m\"\n",
        "    )\n",
        "\n",
        "    # List what was created\n",
        "    print(\"\\nðŸ“‚ Created files:\")\n",
        "    if os.path.exists(output_dir):\n",
        "        for f in os.listdir(output_dir):\n",
        "            size_mb = os.path.getsize(os.path.join(output_dir, f)) / (1024*1024)\n",
        "            print(f\"  - {f} ({size_mb:.1f} MB)\")\n",
        "    print(\"\\nâœ… GGUF export complete!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## 7) Download the GGUF model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!find /content -name \"*.gguf\" -type f 2>/dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Create Ollama model locally\n",
        "\n",
        "After downloading the GGUF file, run on your local machine:\n",
        "\n",
        "```bash\n",
        "# Create a Modelfile\n",
        "echo 'FROM ./pockitect-unsloth.Q4_K_M.gguf' > Modelfile\n",
        "echo 'PARAMETER temperature 0.2' >> Modelfile\n",
        "\n",
        "# Create the Ollama model\n",
        "ollama create pockitect:finetuned -f Modelfile\n",
        "\n",
        "# Test it\n",
        "ollama run pockitect:finetuned \"I need a simple blog server\"\n",
        "```"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
